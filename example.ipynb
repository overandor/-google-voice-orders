{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmZUNLUQ4Vu-"
      },
      "source": [
        "# How to connect a chat bot to your memory service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SO74feW74Vu_",
        "outputId": "99599584-0718-45fa-ada9-20607c3d246b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv(\".env\", override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eS81F8u14Vu_",
        "outputId": "789a95da-5abc-4cf9-bb60-e2a2c4cfc315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langgraph_sdk'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-917946924.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph_sdk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Update to your URL. Copy this from page of ryour LangGraph Deployment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdeployment_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langgraph_sdk'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from langgraph_sdk import get_client\n",
        "\n",
        "# Update to your URL. Copy this from page of ryour LangGraph Deployment\n",
        "deployment_url = \"\"\n",
        "\n",
        "client = get_client(url=deployment_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntd8l5Af4Vu_"
      },
      "source": [
        "## Example Chat Bot\n",
        "\n",
        "The bot fetches user memories my semantic similarity, templates them, then responds!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l67Fxc1Y4VvA",
        "outputId": "78e0d45b-a1df-47fa-9821-db3b052e60b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langgraph'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1057644616.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunnables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunnableConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemorySaver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTART\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStateGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph_sdk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langgraph'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import uuid\n",
        "from datetime import datetime, timezone\n",
        "from typing import List, Optional\n",
        "\n",
        "import langsmith\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import AnyMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.checkpoint import MemorySaver\n",
        "from langgraph.graph import START, StateGraph, add_messages\n",
        "from langgraph_sdk import get_client\n",
        "from pydantic.v1 import BaseModel, Field\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "from memory_service import (\n",
        "    _constants as constants,\n",
        "    _settings as settings,\n",
        "    _utils as utils,\n",
        ")\n",
        "\n",
        "\n",
        "class ChatState(TypedDict):\n",
        "    \"\"\"The state of the chatbot.\"\"\"\n",
        "\n",
        "    messages: Annotated[List[AnyMessage], add_messages]\n",
        "    user_memories: List[dict]\n",
        "\n",
        "\n",
        "class ChatConfigurable(TypedDict):\n",
        "    \"\"\"The configurable fields for the chatbot.\"\"\"\n",
        "\n",
        "    user_id: str\n",
        "    thread_id: str\n",
        "    memory_service_url: str = \"\"\n",
        "    model: str\n",
        "    delay: Optional[float]\n",
        "\n",
        "\n",
        "def _ensure_configurable(config: RunnableConfig) -> ChatConfigurable:\n",
        "    \"\"\"Ensure the configuration is valid.\"\"\"\n",
        "    return ChatConfigurable(\n",
        "        user_id=config[\"configurable\"][\"user_id\"],\n",
        "        thread_id=config[\"configurable\"][\"thread_id\"],\n",
        "        mem_assistant_id=config[\"configurable\"][\"mem_assistant_id\"],\n",
        "        memory_service_url=config[\"configurable\"].get(\n",
        "            \"memory_service_url\", os.environ.get(\"MEMORY_SERVICE_URL\", \"\")\n",
        "        ),\n",
        "        model=config[\"configurable\"].get(\n",
        "            \"model\", \"accounts/fireworks/models/firefunction-v2\"\n",
        "        ),\n",
        "        delay=config[\"configurable\"].get(\"delay\", 60),\n",
        "    )\n",
        "\n",
        "\n",
        "PROMPT = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful and friendly chatbot. Get to know the user!\"\n",
        "            \" Ask questions! Be spontaneous!\"\n",
        "            \"{user_info}\\n\\nSystem Time: {time}\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ").partial(\n",
        "    time=lambda: datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        ")\n",
        "\n",
        "\n",
        "@langsmith.traceable\n",
        "def format_query(messages: List[AnyMessage]) -> str:\n",
        "    \"\"\"Format the query for the user's memories.\"\"\"\n",
        "    # This is quite naive :)\n",
        "    return \" \".join([str(m.content) for m in messages if m.type == \"human\"][-5:])\n",
        "\n",
        "\n",
        "async def query_memories(state: ChatState, config: RunnableConfig) -> ChatState:\n",
        "    \"\"\"Query the user's memories.\"\"\"\n",
        "    configurable: ChatConfigurable = config[\"configurable\"]\n",
        "    user_id = configurable[\"user_id\"]\n",
        "    index = utils.get_index()\n",
        "    embeddings = utils.get_embeddings()\n",
        "\n",
        "    query = format_query(state[\"messages\"])\n",
        "    vec = await embeddings.aembed_query(query)\n",
        "    # You can also filter by memory type, etc. here.\n",
        "    with langsmith.trace(\n",
        "        \"pinecone_query\", inputs={\"query\": query, \"user_id\": user_id}\n",
        "    ) as rt:\n",
        "        response = index.query(\n",
        "            vector=vec,\n",
        "            filter={\"user_id\": {\"$eq\": str(user_id)}},\n",
        "            include_metadata=True,\n",
        "            top_k=10,\n",
        "            namespace=settings.SETTINGS.pinecone_namespace,\n",
        "        )\n",
        "        rt.outputs[\"response\"] = response\n",
        "    memories = []\n",
        "    if matches := response.get(\"matches\"):\n",
        "        memories = [m[\"metadata\"][constants.PAYLOAD_KEY] for m in matches]\n",
        "    return {\n",
        "        \"user_memories\": memories,\n",
        "    }\n",
        "\n",
        "\n",
        "@langsmith.traceable\n",
        "def format_memories(memories: List[dict]) -> str:\n",
        "    \"\"\"Format the user's memories.\"\"\"\n",
        "    if not memories:\n",
        "        return \"\"\n",
        "    # Note Bene: You can format better than this....\n",
        "    memories = \"\\n\".join(str(m) for m in memories)\n",
        "    return f\"\"\"\n",
        "\n",
        "## Memories\n",
        "\n",
        "You have noted the following memorable events from previous interactions with the user.\n",
        "<memories>\n",
        "{memories}\n",
        "</memories>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "async def bot(state: ChatState, config: RunnableConfig) -> ChatState:\n",
        "    \"\"\"Prompt the bot to resopnd to the user, incorporating memories (if provided).\"\"\"\n",
        "    configurable = _ensure_configurable(config)\n",
        "    model = init_chat_model(configurable[\"model\"])\n",
        "    chain = PROMPT | model\n",
        "    memories = format_memories(state[\"user_memories\"])\n",
        "    m = await chain.ainvoke(\n",
        "        {\n",
        "            \"messages\": state[\"messages\"],\n",
        "            \"user_info\": memories,\n",
        "        },\n",
        "        config,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"messages\": [m],\n",
        "    }\n",
        "\n",
        "\n",
        "class MemorableEvent(BaseModel):\n",
        "    \"\"\"A memorable event.\"\"\"\n",
        "\n",
        "    description: str\n",
        "    participants: List[str] = Field(\n",
        "        description=\"Names of participants in the event and their relationship to the user.\"\n",
        "    )\n",
        "\n",
        "\n",
        "async def post_messages(state: ChatState, config: RunnableConfig) -> ChatState:\n",
        "    \"\"\"Query the user's memories.\"\"\"\n",
        "    configurable = _ensure_configurable(config)\n",
        "    langgraph_client = get_client(url=configurable[\"memory_service_url\"])\n",
        "    thread_id = config[\"configurable\"][\"thread_id\"]\n",
        "    # Hash \"memory_{thread_id}\" to get a new uuid5 for the memory id\n",
        "    memory_thread_id = uuid.uuid5(uuid.NAMESPACE_URL, f\"memory_{thread_id}\")\n",
        "    try:\n",
        "        await langgraph_client.threads.get(thread_id=memory_thread_id)\n",
        "    except Exception:\n",
        "        await langgraph_client.threads.create(thread_id=memory_thread_id)\n",
        "\n",
        "    await langgraph_client.runs.create(\n",
        "        memory_thread_id,\n",
        "        assistant_id=configurable[\"mem_assistant_id\"],\n",
        "        input={\n",
        "            \"messages\": state[\"messages\"],  # the service dedupes messages\n",
        "        },\n",
        "        config={\n",
        "            \"configurable\": {\n",
        "                \"user_id\": configurable[\"user_id\"],\n",
        "            },\n",
        "        },\n",
        "        multitask_strategy=\"rollback\",\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"messages\": [],\n",
        "    }\n",
        "\n",
        "\n",
        "builder = StateGraph(ChatState, ChatConfigurable)\n",
        "builder.add_node(query_memories)\n",
        "builder.add_node(bot)\n",
        "builder.add_node(post_messages)\n",
        "builder.add_edge(START, \"query_memories\")\n",
        "builder.add_edge(\"query_memories\", \"bot\")\n",
        "builder.add_edge(\"bot\", \"post_messages\")\n",
        "\n",
        "chat_graph = builder.compile(checkpointer=MemorySaver())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8IAa7Ey04im3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vo8GAtwq4jKk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnDdi-sC4VvA"
      },
      "outputs": [],
      "source": [
        "mem_assistant = await client.assistants.create(\n",
        "    graph_id=\"memory\",\n",
        "    config={\n",
        "        \"configurable\": {\n",
        "            \"delay\": 4,  # seconds wait before considering a thread as \"completed\"\n",
        "            \"schemas\": {\n",
        "                \"MemorableEvent\": {\n",
        "                    \"system_prompt\": \"Extract any memorable events from the user's\"\n",
        "                    \" messages that you would like to remember.\",\n",
        "                    \"update_mode\": \"insert\",\n",
        "                    \"function\": MemorableEvent.schema(),\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R2_nDmO4VvA"
      },
      "outputs": [],
      "source": [
        "# mem_assistant = (await client.assistants.search(graph_id=\"memory\"))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtcAVB_34VvA"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "user_id = str(uuid.uuid4())  # more permanent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltt1Tdyp4VvA"
      },
      "outputs": [],
      "source": [
        "thread_id = str(uuid.uuid4())  # can adjust\n",
        "await client.threads.create(thread_id=thread_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkLNRjkT4VvA"
      },
      "outputs": [],
      "source": [
        "class Chat:\n",
        "    def __init__(self, user_id: str, thread_id: str):\n",
        "        self.thread_id = thread_id\n",
        "        self.user_id = user_id\n",
        "\n",
        "    async def __call__(self, query: str) -> str:\n",
        "        chunks = chat_graph.astream_events(\n",
        "            input={\n",
        "                \"messages\": [(\"user\", query)],\n",
        "            },\n",
        "            config={\n",
        "                \"configurable\": {\n",
        "                    \"user_id\": self.user_id,\n",
        "                    \"thread_id\": self.thread_id,\n",
        "                    \"memory_service_url\": deployment_url,\n",
        "                    \"mem_assistant_id\": mem_assistant[\"assistant_id\"],\n",
        "                    \"delay\": 4,\n",
        "                }\n",
        "            },\n",
        "            version=\"v2\",\n",
        "        )\n",
        "        res = \"\"\n",
        "        async for event in chunks:\n",
        "            if event.get(\"event\") == \"on_chat_model_stream\":\n",
        "                tok = event[\"data\"][\"chunk\"].content\n",
        "                print(tok, end=\"\")\n",
        "                res += tok\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTifyrAQ4VvA"
      },
      "outputs": [],
      "source": [
        "chat = Chat(user_id, thread_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TPPMvfe4VvA"
      },
      "outputs": [],
      "source": [
        "_ = await chat(\"Hi there\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOhmYy_94VvA"
      },
      "outputs": [],
      "source": [
        "_ = await chat(\n",
        "    \"I've been planning a surprise party for my friend steve. \"\n",
        "    \"He has been having a rough month and I want it to be special.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVrvSBUn4VvA"
      },
      "outputs": [],
      "source": [
        "_ = await chat(\n",
        "    \"Steve really likes crocheting. Maybe I can do something with that? Or is that dumb... \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvgnYju54VvA"
      },
      "outputs": [],
      "source": [
        "_ = await chat(\"He's also into capoeira...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6uJHTpm4VvA"
      },
      "outputs": [],
      "source": [
        "_ = await chat(\n",
        "    \"Oh that's a cool idea. One time i took classes from this studio nearby. Wonder if they have any recs. \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEtxNO6L4VvB"
      },
      "outputs": [],
      "source": [
        "_ = await chat(\"Idk. Anyways - how are you doing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntW5Yl4M4VvB"
      },
      "outputs": [],
      "source": [
        "_ = await chat(\"My name is Ken btw\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omrNgYH54VvB"
      },
      "source": [
        "## Convo 2\n",
        "\n",
        "Our memory is configured only to consider a thread \"ready to process\" if has been inactive for a minute.\n",
        "We'll wait for things to populate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCffsdOk4VvB"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "await asyncio.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzKgweTN4VvB"
      },
      "outputs": [],
      "source": [
        "thread_id_2 = uuid.uuid4()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uMqNPPA4VvB"
      },
      "outputs": [],
      "source": [
        "chat2 = Chat(user_id, thread_id_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU9lXGJB4VvB"
      },
      "outputs": [],
      "source": [
        "_ = await chat2(\"Remember me?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IH7jvDt4VvB"
      },
      "outputs": [],
      "source": [
        "_ = await chat2(\"wdy remember??\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KU3ufPQ4VvB"
      },
      "outputs": [],
      "source": [
        "_ = await chat2(\"Oh planning is going alright!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}